# The input section
input {
  file {
    id => "csv023"
    add_field => { "[@metadata][source_type]" => "csv023" }
    path => "/home/ubuntu/intel/feeds/external/hphosts*"
    sincedb_path => "/var/tmp/.sincedb_ext"
    start_position => "beginning"
  }
}

# The filter section
filter {
  if [@metadata][source_type] == "csv023" {

  # Required Fields
  mutate {
    add_field => { "comments" => "hpHosts database with PSH, EMD, and GRM classifications" }
    add_field => { "exportable-to-ids" => "true" }
    add_field => { "occurrences" => 1 }
    add_field => { "threat-analysis" => "2" }
    add_field => { "threat-level-id" => "2" }
    add_field => { "threat-published" => "true" }
    add_field => { "threat-timestamp" => "%{@timestamp}" }
    add_field => { "timestamp" => "%{@timestamp}" }
    add_field => { "threat-uuid" => "" }

    #--------------------------------------------------------------------#
    # Update the fields below:
    #--------------------------------------------------------------------#
    # Category is one of: "Network activity", "Payload delivery",
    # "Artifacts dropped", "External analysis", or "Attribution"
    add_field => { "threat-category" => "Network activity" }

    add_field => { "threat-organization" => "hpHosts" }
    add_field => { "threat-source" => "hpHosts PSH, EMD, and GRM classifications" }

    # Threat Data
    add_field => { "[threat-data][has-meta]" => "false" }
    add_field => { "[threat-data][has-types]" => "false" }
    add_field => { "[threat-data][is-extended]" => "false" }
    add_field => { "[threat-data][type1]" => "HOSTNAME" }
    add_field => { "[threat-data][sub-type1]" => "hostname" }
    add_field => { "[threat-data][value1]" => "" }
    add_field => { "[threat-data][meta1]" => "" }
    add_field => { "[threat-data][type2]" => "" }
    add_field => { "[threat-data][sub-type2]" => "" }
    add_field => { "[threat-data][value2]" => "" }
    add_field => { "[threat-data][meta2]" => "" }

  }

  #--------------------------------------------------------------------#
  #--------------------Add feed-specific actions here:-----------------#
  #--------------------------------------------------------------------#
  # Feed processing MUST result in (atleast) one field: "[threat-data][value1]"
  # Grok "overwrite" must be used to overwrite the above processed fields and to prevent field array creation.

  # Example for the Alienvault feed at http://reputation.alienvault.com/reputation.generic
  csv {
      skip_header => true
      columns => [ "localip", "host" ]
  }

  if [message] =~ /^#/ {
      drop { }
  }

  mutate {
    update => {
      "[threat-data][value1]" => "%{host}"
    }

    remove_field => [ "localip", "host" ]
  }

  # Handle parsing failures
  if "_grokparsefailure" in [tags] or "_csvparsefailure" in [tags] {
    drop { }
  }

  #--------------------------------------------------------------------#
  #--------------------End of feed-specific actions--------------------#
  #--------------------------------------------------------------------#

  mutate {
    convert => {
      "threat-analysis" => "integer"
      "threat-level-id" => "integer"
      "threat-has-types" => "boolean"
      "[threat-data][has-types]" => "boolean"
      "[threat-data][has-meta]" => "boolean"
      "[threat-data][is-extended]" => "boolean"
      "occurrences" => "integer"
    }

    # Add Tag(s) for new documents.
    add_tag => [ "hphosts" ]

    # For existing documents, update tags to indicate information overlaps in feeds.
    # The "extratag" field will be removed in the script in output.
    add_field => { "extratag" => "hphosts"}

    # Remove unused field(s)
    remove_field => [ "message" ]

  }
  }

  # Translations for Codes and Analysis
  translate {
    field => "threat-analysis"
    destination => "threat-analysis-code"
    dictionary => [
        "0", "Initial",
        "1", "Ongoing",
        "2", "Complete"
    ]
  }

  translate {
    field => "threat-level-id"
    destination => "threat-level-code"
    dictionary => [
        "1", "High",
        "2", "Medium",
        "3", "Low",
        "4", "Undefined"
    ]
  }

  # Handle Date Objects (If required)
  # date {
  #   match => [ "date-field", "UNIX" ]
  #   target => "target-field"
  # }

  # Parse IP for GeoIP coordinates
  # Adds an additional "threat-ip" field
  grok {
    match => { "[threat-data][value1]" => "%{IP:threat-geoip}" }
    tag_on_failure => [ ]
  }

  geoip {
    source => "threat-geoip"
    database => "/home/ubuntu/GeoLite2-ASN_20190528/GeoLite2-ASN.mmdb"
    tag_on_failure => [ ]
  }

  geoip {
    source => "threat-geoip"
    tag_on_failure => [ ]
  }

  mutate { remove_field => ["threat-geoip"] }

  uuid {
    target    => "threat-uuid"
    overwrite => true
  }

  # Fingerprinting to remove duplicates
  fingerprint {
    concatenate_sources => true
    source => ["[threat-data][type1]", "[threat-data][value1]"]
    target => "[@metadata][fingerprint]"
    method => "MURMUR3"
  }

  if "_jsonparsefailure" in [tags] {
    drop { }
  }
}

# The output section
output {
#  stdout {
#        codec => rubydebug
#  }
  elasticsearch {
    hosts => localhost
    action => "update"
    doc_as_upsert => true
    # manage_template => true
    script => '
    if (ctx._source.occurrences != null) {
      ctx._source.occurrences++;
    }
    if(ctx._source.tags.contains(params.event.get("extratag")) == false) {
      ctx._source.tags.add("%{extratag}");
    }
    ctx._source.remove("extratag")
    '
    # Replace "document_id" with "fingerprint"
    document_id => "%{[@metadata][fingerprint]}"
    index => "threats"
  }
}
